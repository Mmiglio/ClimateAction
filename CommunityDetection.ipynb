{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset.entities import Entities\n",
    "from modules.dataset.tweets import Tweets\n",
    "from modules.network import HashNet\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_DB_PATH = 'data/db/tweets.json'  # Path to tweets dataset\n",
    "HASHTAGS_DB_PATH = 'data/db/hashtags.json' # Path to hashtags dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define networks container, indexed by periods\n",
    "networks = {\n",
    "    2017: None,\n",
    "    2018: None,\n",
    "    2019: None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836950901495631872</td>\n",
       "      <td>2017-03-01 14:46:59</td>\n",
       "      <td>TT SINGAPORE 22:46\\n1.Hong Kong\\n2.#JointAddre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>836950882528989184</td>\n",
       "      <td>2017-03-01 14:46:54</td>\n",
       "      <td>Letting #snapchat prepare me for the day's uns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>836950869639835649</td>\n",
       "      <td>2017-03-01 14:46:51</td>\n",
       "      <td>\"The bill would require the state to get all o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>836950847380668416</td>\n",
       "      <td>2017-03-01 14:46:46</td>\n",
       "      <td>Style-Lead don't Follow #recycledfashion https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>836950839101116421</td>\n",
       "      <td>2017-03-01 14:46:44</td>\n",
       "      <td>‘Shell knew’: oil giant's 1991 film warned of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id          tweet_date  \\\n",
       "0  836950901495631872 2017-03-01 14:46:59   \n",
       "1  836950882528989184 2017-03-01 14:46:54   \n",
       "2  836950869639835649 2017-03-01 14:46:51   \n",
       "3  836950847380668416 2017-03-01 14:46:46   \n",
       "4  836950839101116421 2017-03-01 14:46:44   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  TT SINGAPORE 22:46\\n1.Hong Kong\\n2.#JointAddre...  \n",
       "1  Letting #snapchat prepare me for the day's uns...  \n",
       "2  \"The bill would require the state to get all o...  \n",
       "3  Style-Lead don't Follow #recycledfashion https...  \n",
       "4  ‘Shell knew’: oil giant's 1991 film warned of ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import tweets dataset\n",
    "tweets = Tweets()\n",
    "tweets.from_json(TWEETS_DB_PATH)\n",
    "tweets.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>entity_index</th>\n",
       "      <th>entity_text</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>entity_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1101574442575167489</td>\n",
       "      <td>48</td>\n",
       "      <td>#ClimateChange</td>\n",
       "      <td>#</td>\n",
       "      <td>0.9615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101574442575167489</td>\n",
       "      <td>49</td>\n",
       "      <td>#Science</td>\n",
       "      <td>#</td>\n",
       "      <td>0.9843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101574446341607424</td>\n",
       "      <td>10</td>\n",
       "      <td>#climatechange</td>\n",
       "      <td>^</td>\n",
       "      <td>0.4659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1101574446341607424</td>\n",
       "      <td>26</td>\n",
       "      <td>#climateemergency</td>\n",
       "      <td>#</td>\n",
       "      <td>0.2876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1101574471247380480</td>\n",
       "      <td>16</td>\n",
       "      <td>#climatechange</td>\n",
       "      <td>N</td>\n",
       "      <td>0.5529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id  entity_index        entity_text entity_tag  \\\n",
       "0  1101574442575167489            48     #ClimateChange          #   \n",
       "1  1101574442575167489            49           #Science          #   \n",
       "2  1101574446341607424            10     #climatechange          ^   \n",
       "3  1101574446341607424            26  #climateemergency          #   \n",
       "4  1101574471247380480            16     #climatechange          N   \n",
       "\n",
       "   entity_conf  \n",
       "0       0.9615  \n",
       "1       0.9843  \n",
       "2       0.4659  \n",
       "3       0.2876  \n",
       "4       0.5529  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = Entities()\n",
    "hashtags.from_json(HASHTAGS_DB_PATH)\n",
    "hashtags.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of hashtags used as search seeds\n",
    "seed_list = [\"#climatechange\", \"#climate\", \"#sdgs\", \"#sustainability\", \"#environment\", \"#globalwarming\"]\n",
    "\n",
    "for period in networks.keys():\n",
    "    # Subset tweets for current period\n",
    "    curr_tweets = Tweets()\n",
    "    curr_tweets.df = tweets.df.loc[tweets.df.tweet_date.dt.year == period].copy()\n",
    "    # Get ids of tweets for current period\n",
    "    curr_tweets_id = curr_tweets.df.tweet_id.unique()\n",
    "    # Subset hashtags and words associated to current tweets\n",
    "    curr_hashtags = Entities()\n",
    "    curr_hashtags.df = hashtags.df.loc[hashtags.df.tweet_id.isin(curr_tweets_id)].copy()\n",
    "    # Lower hashtags\n",
    "    curr_hashtags.df[\"entity_text\"] = curr_hashtags.df[\"entity_text\"].str.lower()\n",
    "    # Remove seeds from hashtag dataset\n",
    "    curr_hashtags.df = curr_hashtags.df.loc[~curr_hashtags.df.entity_text.isin(seed_list)]\n",
    "    #curr_words.df = curr_words.df.loc[~curr_words.df.entity_text.isin(seed_list)]\n",
    "    # Generate a dictionary containing words and hashtags networks for each period\n",
    "    networks[period] = HashNet.from_entities(curr_hashtags)\n",
    "    \n",
    "    del curr_hashtags, curr_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 hashtags network consists in 217 connected components\n",
      "The largest cc corresponds to 77% of total\n",
      "2018 hashtags network consists in 207 connected components\n",
      "The largest cc corresponds to 86% of total\n",
      "2019 hashtags network consists in 189 connected components\n",
      "The largest cc corresponds to 88% of total\n"
     ]
    }
   ],
   "source": [
    "for period in networks.keys():\n",
    "    net_type = \"hashtags\"\n",
    "    # Get the list of connected components sorted by size\n",
    "    cc = networks[period].get_connected_components()\n",
    "    # If there is only one cc\n",
    "    if len(cc) == 1:\n",
    "        print(\"{} {} network is connected\".format(period, net_type))\n",
    "    else:\n",
    "        # Compute the ratio between the size of the largest cc and the sum of all the cc sizes\n",
    "        gc_ratio = int(100*cc[0][\"size\"]/sum([cc[i][\"size\"] for i in range(len(cc))]))\n",
    "        # Print results\n",
    "        print(\"{} {} network consists in {} connected components\".format(period, net_type, len(cc)))\n",
    "        print(\"The largest cc corresponds to {}% of total\".format(gc_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the largest connected components\n",
    "for period in networks.keys():\n",
    "    # Get the largest connected component\n",
    "    lcc = networks[period].get_connected_components()[0][\"component\"]\n",
    "    # Project the network on the lcc\n",
    "    networks[period] = networks[period].project_component(lcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommunitiesLouvain(network, resolution=1.0, threshold = 100):\n",
    "    \"\"\"\n",
    "    Filter communities with less than threshold hashtags\n",
    "    return: \n",
    "        comm: map community_id -> list of hashtags\n",
    "        partitions: map hashtag -> community_id\n",
    "    \"\"\" \n",
    "    # compute best partitions (fixed random state for reproducibility)\n",
    "    partition = community.best_partition(graph=network, weight='weight', resolution=resolution, random_state=100)\n",
    "    size = float(len(set(partition.values())))\n",
    "    print('There are {} communities'.format(size))\n",
    "    \n",
    "    # partition is a dictionary in the form {'hashtag':community_id, ...}\n",
    "    # we want ot transform it in the form {'community_id':[hashtag1, hashtag2, ...]}\n",
    "    communities = {}\n",
    "    for p in partition:\n",
    "        if partition[p] in communities:\n",
    "            communities[partition[p]].append(p)\n",
    "        else:\n",
    "            communities[partition[p]] = [p]\n",
    "\n",
    "    # delete small communities (size(community)<threshold)\n",
    "    communities = {k:v for k, v in communities.items() if len(v)>threshold}\n",
    "    print('-> {} communities remaining after filtering'.format(len(communities)))\n",
    "    return communities, partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Year: 2017 ---\n",
      "There are 30.0 communities\n",
      "-> 6 communities remaining after filtering\n",
      "\n",
      "--- Year: 2018 ---\n",
      "There are 32.0 communities\n",
      "-> 19 communities remaining after filtering\n",
      "\n",
      "--- Year: 2019 ---\n",
      "There are 33.0 communities\n",
      "-> 19 communities remaining after filtering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "communities = {\n",
    "    2017: None,\n",
    "    2018: None,\n",
    "    2019: None\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    2017: {'resolution':1., 'threshold':100},\n",
    "    2018: {'resolution':0.9, 'threshold':100},\n",
    "    2019: {'resolution':0.9, 'threshold':100},\n",
    "}\n",
    "\n",
    "for period in networks.keys():\n",
    "    print(\"--- Year: {} ---\".format(period))\n",
    "    comm, partitions = getCommunitiesLouvain(\n",
    "        networks[period].net,\n",
    "        resolution=parameters[period]['resolution'],\n",
    "        threshold=parameters[period]['threshold']\n",
    "    )\n",
    "    communities[period] = {\n",
    "        'communities': comm,\n",
    "        'partitions': partitions\n",
    "    }\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d, descending=True):\n",
    "    \"\"\"\n",
    "    Sort a dictionary based on items\n",
    "    \"\"\"\n",
    "    return {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "def printCommunitiElemnts(\n",
    "    network, communities, num_communities=5,\n",
    "    community_id=None, metric_name='centrality',\n",
    "    showTopK=5, print_metric=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Print most important nodes based on certain metric\n",
    "    \"\"\"\n",
    "    if metric_name=='centrality':\n",
    "        centrality = nx.degree_centrality(network)\n",
    "        metric = centrality\n",
    "        \n",
    "    if metric_name=='degree':\n",
    "        degree = nx.degree(network, weight='weight')\n",
    "        metric = degree\n",
    "        \n",
    "    # find most relevant (based on metric) terms for each community\n",
    "    if community_id == None:\n",
    "        counter = 1\n",
    "        for k,v in communities.items():\n",
    "            print('---Community: {}, Size: {}---'.format(k, len(v)))\n",
    "            tag_dict = {tag: metric[tag] for tag in v}\n",
    "            tag_dict = sort_dict(tag_dict)\n",
    "            tag_list = list(tag_dict.keys())\n",
    "            for tag in tag_list[:showTopK]:\n",
    "                if print_metric:\n",
    "                    print(tag, ',{}: {:.4f}'.format(metric_name, metric[tag]))\n",
    "                else:\n",
    "                    print(tag)\n",
    "            print()\n",
    "            ## Show only num_communities\n",
    "            if counter == num_communities:\n",
    "                break\n",
    "            counter += 1\n",
    "    else:\n",
    "        k = community_id\n",
    "        if k not in communities:\n",
    "            print(\"Community {} has been discarded (too small)\".format(k))\n",
    "            return\n",
    "        v = communities[community_id]\n",
    "        print('---Community: {}---'.format(k))\n",
    "        tag_dict = {tag: metric[tag] for tag in v}\n",
    "        tag_dict = sort_dict(tag_dict)\n",
    "        tag_list = list(tag_dict.keys())\n",
    "        for tag in tag_list[:showTopK]:\n",
    "            if print_metric:\n",
    "                print(tag, ',{}: {:.4f}'.format(metric_name, metric[tag]))\n",
    "            else:\n",
    "                print(tag)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Community: 1, Size: 146---\n",
      "#business\n",
      "#oil\n",
      "#manufacturing\n",
      "#csr\n",
      "#cars\n",
      "#electricvehicles\n",
      "#emissions\n",
      "\n",
      "---Community: 2, Size: 289---\n",
      "#energy\n",
      "#climatestrike\n",
      "#greennewdeal\n",
      "#renewables\n",
      "#solar\n",
      "#renewableenergy\n",
      "#actonclimate\n",
      "\n",
      "---Community: 3, Size: 226---\n",
      "#sustainable\n",
      "#recycling\n",
      "#recycle\n",
      "#globalgoals\n",
      "#australia\n",
      "#eco\n",
      "#reuse\n",
      "\n",
      "---Community: 4, Size: 242---\n",
      "#nature\n",
      "#pollution\n",
      "#conservation\n",
      "#usa\n",
      "#savetheplanet\n",
      "#oceans\n",
      "#trees\n",
      "\n",
      "---Community: 5, Size: 215---\n",
      "#health\n",
      "#ecofriendly\n",
      "#vegan\n",
      "#natural\n",
      "#organic\n",
      "#thursdaythoughts\n",
      "#nutrition\n",
      "\n",
      "---Community: 6, Size: 317---\n",
      "#innovation\n",
      "#ai\n",
      "#technology\n",
      "#iot\n",
      "#news\n",
      "#circulareconomy\n",
      "#plasticfree\n",
      "\n",
      "---Community: 7, Size: 217---\n",
      "#un\n",
      "#sustainabledevelopment\n",
      "#india\n",
      "#peace\n",
      "#sdg\n",
      "#nigeria\n",
      "#unitednations\n",
      "\n",
      "---Community: 8, Size: 306---\n",
      "#climateaction\n",
      "#fridaysforfuture\n",
      "#climatecrisis\n",
      "#extinctionrebellion\n",
      "#climateactionnow\n",
      "#schoolstrike4climate\n",
      "#climateemergency\n",
      "\n",
      "---Community: 9, Size: 150---\n",
      "#water\n",
      "#policy\n",
      "#politics\n",
      "#us\n",
      "#california\n",
      "#drought\n",
      "#life\n",
      "\n",
      "---Community: 10, Size: 173---\n",
      "#parisagreement\n",
      "#genderequality\n",
      "#2030agenda\n",
      "#unea4\n",
      "#cycloneidai\n",
      "#sdg13\n",
      "#sdg14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year = 2019\n",
    "\n",
    "printCommunitiElemnts(\n",
    "    networks[year].net, communities[year]['communities'],\n",
    "    num_communities=10, metric_name='centrality',\n",
    "    print_metric=False, showTopK=7, community_id=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Community: 8---\n",
      "#climateaction\n",
      "#fridaysforfuture\n",
      "#climatecrisis\n",
      "#extinctionrebellion\n",
      "#climateactionnow\n",
      "#schoolstrike4climate\n",
      "#climateemergency\n",
      "#gretathunberg\n",
      "#iwd2019\n",
      "#fridayforfuture\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect community by id\n",
    "year = 2019\n",
    "\n",
    "printCommunitiElemnts(\n",
    "    networks[year].net, communities[year]['communities'],\n",
    "    num_communities=10, metric_name='centrality',\n",
    "    print_metric=False, showTopK=10, community_id=8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
