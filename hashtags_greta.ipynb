{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/laura/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/laura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.words import load_data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Constants\n",
    "years = [2018, 2019]\n",
    "\n",
    "# Load tweets dataset table\n",
    "tweets = load_data('greta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count occurrences of hashtags used as seeds\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_row(row, words_dict):\n",
    "    \"\"\" \n",
    "    Conta una sola volta eventuali hashtag ripetuti. Bug o ok?\n",
    "    \"\"\"\n",
    "    # Find the set of hashtags present in the tweet\n",
    "    text = row.text.lower().replace(\"#\", \" #\")\n",
    "    hash_tweet = set(re.findall(r\"#(\\w+)\", text))\n",
    "    hash_dict = set(words_dict.keys())\n",
    "    # Update the number of hashtag in the dictionary\n",
    "    for word in (hash_tweet & hash_dict):\n",
    "        words_dict[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "def count_words_in_list(df, words_list):\n",
    "    # Initialize the dictionary to store counts\n",
    "    words_count = dict(zip(words_list, [0]*len(words_list)))\n",
    "    # Scan each row updating the counters\n",
    "    df.apply(count_words_in_row, words_dict=words_count, axis=1)\n",
    "    \n",
    "    return words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st line: neutral hashtags\n",
    "# 2nd line: specific hastags\n",
    "words_list=[\"climatechange\", \"climatecrisis\", \"parisagreement\", \n",
    "            \"gretathunberg\", \"climatestrike\", \"fridays4future\"]\n",
    "                    \n",
    "datasets = [tweets[tweets.created_at.dt.year == years[0]],\n",
    "            tweets[tweets.created_at.dt.year == years[1]]]\n",
    "            \n",
    "for i, data in enumerate(datasets):\n",
    "    print('hashtag frequencies in {}: \\n\\n'.format(years[i]), count_words_in_list(data, words_list), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find occurrences of all the used hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hash_distribution(row, d):\n",
    "    # Find all the hashtag in the tweet\n",
    "    hash_list = re.findall(r\"#(\\w+)\", row.text.lower())\n",
    "    \n",
    "    # Increase the counter or define a new key with value 1\n",
    "    for hashtag in hash_list:\n",
    "        if hashtag in d: \n",
    "            d[hashtag] += 1\n",
    "        else:\n",
    "            d[hashtag] = 1\n",
    "\n",
    "\n",
    "\n",
    "def plot_hash_distribution(df, n_hash_max, n_hash_min=0):\n",
    "    # Initialize the dictionary to store counts\n",
    "    hash_dict = {}\n",
    "    # Scan each row of the df\n",
    "    df.apply(find_hash_distribution, d=hash_dict, axis=1)\n",
    "    \n",
    "    # Convert the dictionary in a df, sort it by frequency\n",
    "    hash_df = pd.DataFrame.from_dict(hash_dict.items())\n",
    "    hash_df.columns = [\"hashtag\", \"occurrences\"]\n",
    "    hash_df.sort_values(by=\"occurrences\", ascending=False, inplace=True)\n",
    "\n",
    "    # Plot the barplot with the specified interval of the most frequent hashtags\n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "    hash_df[n_hash_min : n_hash_max].plot(x=\"hashtag\", y=\"occurrences\", kind=\"bar\", ax = ax)\n",
    "    plt.show()\n",
    "    \n",
    "    return hash_df\n",
    "\n",
    "\n",
    "\n",
    "def define_mask(row, mask, words):\n",
    "    hash_tweet = set(re.findall(r\"#(\\w+)\", row.text.lower()))\n",
    "    mask.append((hash_tweet & words) != set())\n",
    "\n",
    "    \n",
    "\n",
    "def filter_dataset(df, words, year, n_max=25, n_min=1):\n",
    "    \"\"\"\n",
    "    Return a dataframe that contains tweets in which is present at least \n",
    "    one hashtags in list \"words\" written in the specified year.\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    if words:\n",
    "        tweets.apply(define_mask, mask=mask, words=set(words), axis=1)\n",
    "    else:\n",
    "        mask = True\n",
    "    \n",
    "    count_words_in_list(tweets[mask & (tweets.created_at.dt.year == year)], words_list)\n",
    "    top_hash = plot_hash_distribution(tweets[mask & (tweets.created_at.dt.year == 2019)], \n",
    "                       n_hash_max=n_max, n_hash_min=n_min)\n",
    "    \n",
    "    return top_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hash_pre = filter_dataset(tweets, words=[], year=2018, n_max=25, n_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hash_post = filter_dataset(tweets, words=[], year=2019, n_max=25, n_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_hash_post[top_hash_post.hashtag.isin(words_list)]\n",
    "top_hash_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(top_hash_pre.shape)\n",
    "#print(top_hash_pre.hashtag.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_hash_post.shape)\n",
    "#print(top_hash_post.hashtag.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(top_hash_post.hashtag.values.tolist()) - set(top_hash_pre.hashtag.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(top_hash_pre.hashtag.values.tolist()) - set(top_hash_post.hashtag.values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency Check\n",
    "### Verify if the output of a random line is coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "text = tweets.loc[index].text\n",
    "hash_list = re.findall(r\"#(\\w+)\", text.lower())\n",
    "\n",
    "print(\"TEXT: \", text, \"\\n\\nHASHTAG LIST: \", hash_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with the original json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "import json\n",
    "\n",
    "PATH_JSON = \"data/postGreta.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    \"\"\"\n",
    "    Parse tweet hashtags\n",
    "    \"\"\"\n",
    "    return [t['text'].lower() for t in tweet['entities']['hashtags']]\n",
    "\n",
    "\n",
    "\n",
    "def load_jsonl(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        # extract relevant fields from tweets. Be aware that replies\n",
    "        # have a different structure. For example, assuming we would\n",
    "        # like to extract hashtags we need to distinguish between different cases\n",
    "        # (other fields return truncated text and hastags)\n",
    "        tweets_hashtags = {}\n",
    "        for tweet in json_lines.reader(f, broken=True):\n",
    "            # tweet is a reply\n",
    "            if 'retweeted_status' in tweet:\n",
    "                try:\n",
    "                    final_tweet = tweet['retweeted_status']['extended_tweet']\n",
    "                except:\n",
    "                    # text and hashtags have not been truncated\n",
    "                    final_tweet = tweet['retweeted_status']\n",
    "            # no reply\n",
    "            else:\n",
    "                try:\n",
    "                    final_tweet = tweet['extended_tweet']\n",
    "                except:\n",
    "                    final_tweet = tweet\n",
    "            tweets_hashtags[tweet['id']] = get_hashtags(final_tweet)\n",
    "    \n",
    "    return tweets_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_post = load_jsonl(PATH_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ghost_id = []\n",
    "error_id = []\n",
    "\n",
    "for i in range(1000):\n",
    "    tweet_id = random.choice(list(hashtags_post.keys()))\n",
    "    \n",
    "    text = None\n",
    "    try:\n",
    "        text = tweets[tweets.id == str(tweet_id)].text.values[0]\n",
    "    except:\n",
    "        ghost_id.append(tweet_id)\n",
    "        \n",
    "    if text:\n",
    "        hash_list_original = hashtags_post[tweet_id]\n",
    "        hash_list_extracted = re.findall(r\"#(\\w+)\", text.lower())\n",
    "        \n",
    "        if hash_list_original != hash_list_extracted:\n",
    "            error_id.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ghost_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets.id == ghost_id[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
